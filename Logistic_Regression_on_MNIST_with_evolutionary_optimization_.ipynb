{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalida1wwin/CMPUT-328/blob/main/Logistic_Regression_on_MNIST_with_evolutionary_optimization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHPwL1QYcQqU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "def compute_score(acc, min_thres, max_thres):\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return base_score\n",
        "\n",
        "\n",
        "def run(algorithm, dataset_name, filename):\n",
        "    start = timeit.default_timer()\n",
        "    predicted_test_labels, gt_labels = algorithm(dataset_name)\n",
        "    if predicted_test_labels is None or gt_labels is None:\n",
        "      return (0, 0, 0)\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - start\n",
        "    \n",
        "    np.savetxt(filename, np.asarray(predicted_test_labels))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for label, prediction in zip(gt_labels, predicted_test_labels):\n",
        "      total += label.size(0)\n",
        "      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n",
        "      \n",
        "    accuracy = float(correct) / total\n",
        "    \n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    return (correct, accuracy, run_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "# from CIFAR10_Multiple_Linear_Regression.ipynb on eclass\n",
        "CIFAR10_batch_size_train = 200\n",
        "MNIST_batch_size_train = 270\n",
        "batch_size_test = 1000\n",
        "from torch.utils.data import random_split\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "CIFAR10_training = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# create a training and a validation set\n",
        "CIFAR10_training_set, CIFAR10_validation_set = random_split(CIFAR10_training, [38000, 12000])\n",
        "\n",
        "# CIFAR-10 test set\n",
        "CIFAR10_test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "CIFAR10_train_loader = torch.utils.data.DataLoader(CIFAR10_training_set,\n",
        "                                           batch_size=CIFAR10_batch_size_train,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "CIFAR10_validation_loader = torch.utils.data.DataLoader(CIFAR10_validation_set,\n",
        "                                                batch_size=CIFAR10_batch_size_train,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "CIFAR10_test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,\n",
        "                                          batch_size=batch_size_test, \n",
        "                                          shuffle=False, num_workers=2)\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "#MNIST dataset is part of torchvision\n",
        "\n",
        "MNIST_training = datasets.MNIST('/MNIST_dataset/', train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "MNIST_test_set = datasets.MNIST('/MNIST_dataset/', train=False, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "# create a training and a validation set\n",
        "MNIST_training_set, MNIST_validation_set = random_split(MNIST_training, [48000, 12000])\n",
        "\n",
        "\n",
        "MNIST_train_loader = torch.utils.data.DataLoader(MNIST_training_set,batch_size=MNIST_batch_size_train, shuffle=True)\n",
        "\n",
        "MNIST_validation_loader = torch.utils.data.DataLoader(MNIST_validation_set,batch_size=MNIST_batch_size_train, shuffle=True)\n",
        "\n",
        "MNIST_test_loader = torch.utils.data.DataLoader(MNIST_test_set,batch_size=batch_size_test, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqcKTq63-JG3",
        "outputId": "943321b9-116e-4dab-b305-b8d1669ab1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MNIST_test_set.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nXk7iF1kqTG",
        "outputId": "34795d8a-ff9b-4fbf-a759-9863b6dae665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from CIFAR10_Multiple_Linear_Regression.ipynb on eclass\n",
        "# from torch.utils.data import random_split\n",
        "\n",
        "# momentum = 0.5\n",
        "log_interval = 100\n",
        "CIFAR10_n_epochs = int(3000 /(38000/CIFAR10_batch_size_train))\n",
        "MNIST_n_epochs = int(3800 /(48000/CIFAR10_batch_size_train))\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Checking GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(CIFAR10_n_epochs)\n",
        "print(MNIST_n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFgpwL3s-iNh",
        "outputId": "67878770-06f2-40ba-ac9d-d821e52ff629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "15\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(CIFAR10_test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)\n",
        "print(example_targets.shape)\n",
        "\n",
        "examples = enumerate(MNIST_test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)\n",
        "print(example_targets.shape)\n",
        "# print(example_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIkGSuF7_BAW",
        "outputId": "b6877303-5e2f-43e3-e881-ab45bcca0294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 3, 32, 32])\n",
            "torch.Size([1000])\n",
            "torch.Size([1000, 1, 28, 28])\n",
            "torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "MAnimj1Ka4OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple Linear regression\n",
        "class CIFAR10LogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(3*32*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Softmax = nn.Softmax(dim=1)\n",
        "        # x = Softmax(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "class MNISTLogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTLogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(1*28*28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Softmax = nn.Softmax(dim=1)\n",
        "        # x = Softmax(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zfe1sffIayqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class One_Hot(nn.Module):\n",
        "    def __init__(self, depth):\n",
        "        super(One_Hot,self).__init__()\n",
        "        self.depth = depth\n",
        "        self.ones = torch.sparse.torch.eye(depth).to(device)\n",
        "    def forward(self, X_in):\n",
        "        X_in = X_in.long()\n",
        "        return self.ones.index_select(0,X_in.data)\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"({})\".format(self.depth)"
      ],
      "metadata": {
        "id": "gtiY-xidfb9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(multi_linear_model,validation_loader):\n",
        "  multi_linear_model.eval()\n",
        "  validation_loss = 0\n",
        "  correct = 0\n",
        "  one_hot = One_Hot(10).to(device)\n",
        "  with torch.no_grad(): # notice the use of no_grad\n",
        "    for data, target in validation_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = multi_linear_model(data)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      ####\n",
        "      CE = nn.CrossEntropyLoss()\n",
        "      loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "      validation_loss +=loss\n",
        "      ####\n",
        "      # validation_loss += F.mse_loss(output, one_hot(target), size_average=False).item()\n",
        "\n",
        "  validation_loss /= len(validation_loader.dataset)\n",
        "  Accuracy = 100. * correct / len(validation_loader.dataset)\n",
        "  print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(validation_loss, correct, len(validation_loader.dataset), 100. * correct / len(validation_loader.dataset)))"
      ],
      "metadata": {
        "id": "TO667fjjEcaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(multi_linear_model,test_loader):\n",
        "  multi_linear_model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  one_hot = One_Hot(10).to(device)\n",
        "  # gt_labels_tensor = torch.zeros(len(test_loader),dtype=torch.float,device= device)\n",
        "  final_target = torch.FloatTensor([]).to(device)\n",
        "  final_pred = torch.FloatTensor([]).to(device)\n",
        "  with torch.no_grad():\n",
        "    for i, (data, target) in enumerate(test_loader):\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      final_target = torch.cat([final_target, target]).view(-1).to(device)\n",
        "      \n",
        "      # print(target)\n",
        "      # print(target.shape)\n",
        "      # gt_labels_tensor[i] = target\n",
        "      \n",
        "      output = multi_linear_model(data)\n",
        "      ####\n",
        "      CE = nn.CrossEntropyLoss()\n",
        "      loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "      test_loss +=loss\n",
        "      ####\n",
        "      # test_loss += F.mse_loss(output, one_hot(target), size_average=False).item()\n",
        "      \n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      # print(pred)\n",
        "      final_pred = torch.cat([final_pred, pred.view_as(target)])\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "      \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "  # print(final_pred.shape, final_target.shape)\n",
        "  # print(final_pred, final_target)\n",
        "  return final_pred, final_target"
      ],
      "metadata": {
        "id": "i0WSdITjRobF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Mjmw05cQq0"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(dataset_name):\n",
        "    # epoch = 5\n",
        "    log_interval = 100\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if dataset_name == \"CIFAR10\":\n",
        "      # Lambda = 0.001\n",
        "      # learning_rate = 0.0005\n",
        "      Lambda = Lambda_CIFAR10\n",
        "      learning_rate = learning_rate_CIFAR10\n",
        "      LogisticRegression = CIFAR10LogisticRegression().to(device)\n",
        "      # optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95) # the best\n",
        "      # optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      if optimizerName_CIFAR10 == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName_CIFAR10 == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      # validation(LogisticRegression,CIFAR10_validation_loader)\n",
        "      for epoch in range(CIFAR10_n_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(CIFAR10_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # validation(LogisticRegression,CIFAR10_validation_loader)\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(CIFAR10_train_loader.dataset),\n",
        "              100. * batch_idx / len(CIFAR10_train_loader), loss.item()))\n",
        "        if epoch % 5 == 0:\n",
        "          validation(LogisticRegression,CIFAR10_validation_loader)\n",
        "      # test after training \n",
        "      predicted_test_labels , gt_labels_tensor = test(LogisticRegression,CIFAR10_test_loader)\n",
        "      gt_labels = CIFAR10_test_set.targets\n",
        "    elif dataset_name == \"MNIST\":\n",
        "      # Lambda = 0.0001\n",
        "      # learning_rate = 0.001\n",
        "      Lambda = Lambda_MNIST\n",
        "      learning_rate = learning_rate_MNIST\n",
        "      LogisticRegression = MNISTLogisticRegression().to(device)\n",
        "      # optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      # optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate) # The best \n",
        "      if optimizerName_MNIST == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName_MNIST == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      # validation(LogisticRegression,MNIST_validation_loader)\n",
        "      for epoch in range(MNIST_n_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(MNIST_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # validation(LogisticRegression,MNIST_validation_loader)\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(MNIST_train_loader.dataset),\n",
        "              100. * batch_idx / len(MNIST_train_loader), loss.item()))\n",
        "        if epoch % 5 == 0:\n",
        "          validation(LogisticRegression,MNIST_validation_loader)\n",
        "      # test after training \n",
        "      predicted_test_labels,gt_labels_tensor = test(LogisticRegression,MNIST_test_loader)\n",
        "    return predicted_test_labels.view(1000,10).cpu() , gt_labels_tensor.view(1000,10).cpu() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfDNwazz1RFw"
      },
      "outputs": [],
      "source": [
        "def tune_hyper_parameter():\n",
        "    # TODO: implement logistic regression hyper-parameter tuning here\n",
        "    # learning rate and lambda\n",
        "    startTime = timeit.default_timer()\n",
        "    params_to_tune = [{\"lr\": 0.01 , \"lambda\": 0.001}, \n",
        "                      {\"lr\": 0.01 , \"lambda\": 0.0001},\n",
        "                      {\"lr\": 0.001, \"lambda\": 0.001}, \n",
        "                      {\"lr\": 0.001, \"lambda\": 0.0001},\n",
        "                      {\"lr\": 0.01 , \"lambda\": 0.01},\n",
        "                      {\"lr\": 0.0005, \"lambda\": 0.01}, \n",
        "                      {\"lr\": 0.0005, \"lambda\": 0.001}]\n",
        "    \n",
        "    best_accuracy = 0.0\n",
        "    best_params = None\n",
        "    filenames = { \"MNIST\": \"predictions_mnist_KhalidAlmahrezi_1580848.txt\", \"CIFAR10\": \"predictions_cifar10_KhalidAlmahrezi_1580848.txt\"}\n",
        "\n",
        "    final_best_best_params = {\"Adam\":{\"lr\":0.0 , \"lambda\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0, \"lambda\": 0.0}}\n",
        "    global learning_rate \n",
        "    global Lambda \n",
        "    global optimizerName \n",
        "    for dataset_name in [\"CIFAR10\", \"MNIST\"]:\n",
        "    # for dataset_name in [ \"MNIST\", \"CIFAR10\"]:\n",
        "      for optimizerName in [\"Adam\",\"SGD\"]:\n",
        "        for params in params_to_tune:\n",
        "          learning_rate = params[\"lr\"]\n",
        "          Lambda = params[\"lambda\"]\n",
        "          print(\"optimizer Name: \", optimizerName, \"| learning rate = \", learning_rate, \"| Lambda = \", Lambda)\n",
        "        # global params    # Specify params to search as a global variable, to be used for logistic_regression, also feel free to add more arguments to all existing functions\n",
        "          result, score = run_on_dataset_for_tuning(dataset_name, filenames[dataset_name])\n",
        "          if result[\"accuracy\"] > best_accuracy:\n",
        "              best_accuracy = result[\"accuracy\"]\n",
        "              best_params = params\n",
        "              final_best_best_params[optimizerName][\"lr\"] = learning_rate\n",
        "              final_best_best_params[optimizerName][\"lambda\"] = Lambda\n",
        "          print(best_params, best_accuracy)\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - startTime\n",
        "    print(best_params, best_accuracy, run_time)\n",
        "    print(final_best_best_params, best_accuracy, run_time)\n",
        "\n",
        "    return final_best_best_params, best_accuracy, run_time\n",
        "    \n",
        "    # return None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import heapq\n",
        "# rng_seed = np.random.default_rng(143341)\n",
        "# LR = (rng_seed.integers(low=0, high=1000, size=3)/10000) \n",
        "# Lammda =  (rng_seed.integers(low=0, high=1000, size=3)/100000) \n",
        "\n",
        "# LR_2d = []\n",
        "# Lammda_2d = []\n",
        "# for i in range(len(LR)):\n",
        "#   heapq.heappush(LR_2d,([i,LR[i]]))\n",
        "#   heapq.heappush(Lammda_2d,([i,Lammda[i]]))\n",
        "# # for i in [0.91, 0.82, 0.89]:\n",
        "\n",
        "\n",
        "# print(\"lammda\",Lammda_2d,\"LR\",LR_2d)\n",
        "# print(Lammda_2d,LR_2d)\n",
        "# print(heapq.heappop(Lammda_2d))\n",
        "# print(heapq.heappop(LR_2d))\n",
        "# print(\"lammda\",Lammda_2d,\"LR\",LR_2d)\n",
        "# print((rng_seed.integers(low=0, high=1000, size=1)/10000)[0])\n",
        "# print(LR_2d)\n",
        "# print(Lammda_2d)\n",
        "# sorted(Lammda_2d)\n",
        "# # heapq.heapify([el * -1 for el in LR_2d ])\n",
        "\n",
        "# print(LR_2d)\n",
        "# print(Lammda_2d)\n",
        "# print(LR_2d.pop())\n",
        "# print(Lammda_2d.pop())\n",
        "# print(LR_2d)\n",
        "# print(Lammda_2d)"
      ],
      "metadata": {
        "id": "7OWiMwhCxTqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyper_parameter():\n",
        "    # TODO: implement logistic regression hyper-parameter tuning here\n",
        "    # learning rate and lambda\n",
        "    # with Evolutionary optimization from\n",
        "    # https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization\n",
        "    startTime = timeit.default_timer()\n",
        "    import numpy as np\n",
        "    import heapq\n",
        "    \n",
        "    div = 1000000\n",
        "    GenSize = 3\n",
        "    remove = 2\n",
        "    best_accuracy = 0.0\n",
        "    genarations = 4\n",
        "    \n",
        "\n",
        "    \n",
        "    final_best_best_params = {\n",
        "                      \"MNIST\":{\"Adam\":{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":None},\n",
        "                              \n",
        "                    \"CIFAR10\":{\"Adam\":{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":None}}\n",
        "    best_params = None\n",
        "    filenames = { \"MNIST\": \"predictions_mnist_KhalidAlmahrezi_1580848.txt\", \"CIFAR10\": \"predictions_cifar10_KhalidAlmahrezi_1580848.txt\"}\n",
        "    global learning_rate \n",
        "    global Lambda \n",
        "    global optimizerName\n",
        "    global epochs  \n",
        "    # for dataset_name in [\"CIFAR10\", \"MNIST\"]:\n",
        "    for dataset_name in [ \"MNIST\", \"CIFAR10\"]:\n",
        "      for optimizerName in [\"Adam\",\"SGD\"]:\n",
        "        # 1 Create an initial population of random solutions\n",
        "        rng_seed = np.random.default_rng(143341)\n",
        "        \n",
        "        rng_seed = np.random.default_rng(143341)\n",
        "        learning_rateGen1 = rng_seed.integers(low=0, high=10000, size=GenSize)/div\n",
        "        LammdaGen1 =  rng_seed.integers(low=0, high=10000, size=GenSize)/div\n",
        "        LR_2d = []\n",
        "        Lammda_2d = []\n",
        "        print(\"learning_rateGen1\")\n",
        "        print(learning_rateGen1)\n",
        "        print(\"LammdaGen1\")\n",
        "        print(LammdaGen1)\n",
        "        for i in range(GenSize):\n",
        "          heapq.heappush(LR_2d,([0,learning_rateGen1[i]]))\n",
        "          heapq.heappush(Lammda_2d,([0,LammdaGen1[i]]))\n",
        "\n",
        "        for gen in range(genarations):\n",
        "          print(\"gen:\", gen)\n",
        "          newLR_2d = []\n",
        "          newLammda_2d = []\n",
        "          # start of the generations\n",
        "          for i in range(GenSize):\n",
        "            # 2 Evaluate the hyperparameters tuples and acquire their fitness function\n",
        "            learning_rate = LR_2d[i][1]\n",
        "            Lambda = Lammda_2d[i][1]\n",
        "            print(\"optimizer Name: \", optimizerName, \"| learning rate = \", learning_rate, \"| Lambda = \", Lambda)\n",
        "            if gen == 0: \n",
        "              epochs = 1\n",
        "            elif gen == 3: \n",
        "              epochs = 3\n",
        "            # elif gen == 5:\n",
        "            #   epochs = 4\n",
        "            # elif gen == 6:\n",
        "            #   epochs = 6\n",
        "          #  global params    # Specify params to search as a global variable, to be used for logistic_regression, also feel free to add more arguments to all existing functions\n",
        "            result, score = run_on_dataset_for_tuning(dataset_name, filenames[dataset_name])\n",
        "            if result[\"accuracy\"] > final_best_best_params[dataset_name][optimizerName][\"best_accuracy\"]:\n",
        "              best_accuracy = result[\"accuracy\"]\n",
        "              final_best_best_params[dataset_name][optimizerName][\"best_accuracy\"] =  result[\"accuracy\"]\n",
        "              final_best_best_params[dataset_name][\"best_optimizer\"] =  optimizerName\n",
        "              best_params = {learning_rate,Lambda}\n",
        "              # 3 Rank the hyperparameter tuples by their relative fitness\n",
        "              heapq.heappush(newLR_2d,[result[\"accuracy\"],learning_rate])\n",
        "              heapq.heappush(newLammda_2d,[result[\"accuracy\"],Lambda])\n",
        "              final_best_best_params[dataset_name][optimizerName][\"lr\"] = learning_rate\n",
        "              final_best_best_params[dataset_name][optimizerName][\"lambda\"] = Lambda \n",
        "              # print(best_params, best_accuracy)\n",
        "            else:\n",
        "              heapq.heappush(newLR_2d,[result[\"accuracy\"],learning_rate])\n",
        "              heapq.heappush(newLammda_2d,[result[\"accuracy\"],Lambda])\n",
        "\n",
        "\n",
        "            # 4 Replace the worst-performing hyperparameter tuples with new hyperparameter tuples generated\n",
        "            print(\"newLR_2d\", newLR_2d,\"newLammda_2d\",newLammda_2d,)\n",
        "\n",
        "          # select the best two acc two replace the worst acc\n",
        "          for n in range(remove):\n",
        "            sorted_LR_2d = sorted(newLR_2d.copy())\n",
        "            sortedLimts_LR_2d = []\n",
        "            heapq.heappush(sortedLimts_LR_2d, sorted_LR_2d.pop()[1])\n",
        "            heapq.heappush(sortedLimts_LR_2d, sorted_LR_2d.pop()[1])\n",
        "            High_LR_2d = sortedLimts_LR_2d[1]\n",
        "            Low_LR_2d = sortedLimts_LR_2d[0]\n",
        "\n",
        "\n",
        "\n",
        "            sorted_Lammda_2d = sorted(newLammda_2d.copy())\n",
        "            sortedLimts_Lammda_2d = []\n",
        "            heapq.heappush(sortedLimts_Lammda_2d, sorted_Lammda_2d.pop()[1])\n",
        "            heapq.heappush(sortedLimts_Lammda_2d, sorted_Lammda_2d.pop()[1])\n",
        "            High_Lammda_2d = sortedLimts_Lammda_2d[1]\n",
        "            Low_Lammda_2d = sortedLimts_Lammda_2d[0]\n",
        "            # print(sortedlimts_LR_2d)\n",
        "            print(\"low= \",Low_LR_2d, \"high=\",High_LR_2d)\n",
        "            # print(sortedlimts_Lammda_2d)\n",
        "            print(\"low= \",Low_Lammda_2d, \"high=\",High_Lammda_2d)\n",
        "            heapq.heappop(newLR_2d)\n",
        "            heapq.heappop(newLammda_2d)\n",
        "            if High_LR_2d != Low_LR_2d:\n",
        "              heapq.heappush(newLR_2d, [0, (rng_seed.integers(low=Low_LR_2d* div, high=High_LR_2d* div, size=1)/div)[0]])\n",
        "            else:\n",
        "              heapq.heappush(newLR_2d, [0, High_LR_2d/2])\n",
        "            \n",
        "            if High_Lammda_2d != Low_Lammda_2d:  \n",
        "              heapq.heappush(newLammda_2d, [0, (rng_seed.integers(low=Low_Lammda_2d * div, high=High_Lammda_2d* div, size=1)/div)[0]])\n",
        "            else:\n",
        "              heapq.heappush(newLammda_2d, [0, High_Lammda_2d/2])\n",
        "            # update the new gen list\n",
        "            # print(\"befor heapify\",newLR_2d,newLammda_2d)\n",
        "            # heapq.heapify(newLR_2d)\n",
        "            # heapq.heapify(newLammda_2d)\n",
        "            # print(\"after heapify\",newLR_2d,newLammda_2d)\n",
        "            LR_2d = newLR_2d\n",
        "            Lammda_2d = newLammda_2d\n",
        "          print(\"LR_2d\", LR_2d,\"Lammda_2d\",Lammda_2d,)\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - startTime\n",
        "    print(final_best_best_params, best_accuracy, run_time)\n",
        "    return final_best_best_params, best_accuracy, run_time\n"
      ],
      "metadata": {
        "id": "nYBsROwupNqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_on_dataset_for_tuning(dataset_name, filename):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        min_thres = 0.82\n",
        "        max_thres = 0.92\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        min_thres = 0.28\n",
        "        max_thres = 0.38\n",
        "\n",
        "    correct_predict, accuracy, run_time = run(logistic_regression_for_tuning, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                         accuracy=accuracy, score=score,\n",
        "                         run_time=run_time)\n",
        "    return result, score"
      ],
      "metadata": {
        "id": "0EsLhrDAI2Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_for_tuning(multi_linear_model,validation_loader):\n",
        "  multi_linear_model.eval()\n",
        "  validation_loss = 0\n",
        "  correct = 0\n",
        "  one_hot = One_Hot(10).to(device)\n",
        "  final_target = torch.FloatTensor([]).to(device)\n",
        "  final_pred = torch.FloatTensor([]).to(device)\n",
        "  with torch.no_grad(): \n",
        "    for data, target in validation_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      target = target.to(device)\n",
        "      final_target = torch.cat([final_target, target]).view(-1).to(device)\n",
        "      output = multi_linear_model(data)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      final_pred = torch.cat([final_pred, pred.view_as(target)])\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      CE = nn.CrossEntropyLoss()\n",
        "      loss = CE(output, one_hot(target))\n",
        "      validation_loss +=loss\n",
        "      \n",
        "  validation_loss /= len(validation_loader.dataset)\n",
        "  Accuracy = 100. * correct / len(validation_loader.dataset)\n",
        "  print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(validation_loss, correct, len(validation_loader.dataset), 100. * correct / len(validation_loader.dataset)))\n",
        "\n",
        "  return final_pred, final_target"
      ],
      "metadata": {
        "id": "VnXxu2Xkz8Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_for_tuning(dataset_name):\n",
        "\n",
        "    print(\"In the logistic_regression_for_tuning\", \"optimizerName: \", optimizerName, \"| learning rate = \", learning_rate, \"| Lambda = \", Lambda)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if dataset_name == \"CIFAR10\":\n",
        "      LogisticRegression = CIFAR10LogisticRegression().to(device)\n",
        "      if optimizerName == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      for epoch in range(epochs):\n",
        "        for batch_idx, (data, target) in enumerate(CIFAR10_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) \n",
        "\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(CIFAR10_train_loader.dataset),\n",
        "              100. * batch_idx / len(CIFAR10_train_loader), loss.item()))\n",
        "        predicted_test_labels,gt_labels_tensor = validation_for_tuning(LogisticRegression,CIFAR10_test_loader)\n",
        "        predicted_test_labels = predicted_test_labels.view(1000,10).cpu()\n",
        "        gt_labels_tensor = gt_labels_tensor.view(1000,10).cpu() \n",
        "    elif dataset_name == \"MNIST\":\n",
        "      LogisticRegression = MNISTLogisticRegression().to(device)\n",
        "      if optimizerName == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      for epoch in range(epochs):\n",
        "        for batch_idx, (data, target) in enumerate(MNIST_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(MNIST_train_loader.dataset),\n",
        "              100. * batch_idx / len(MNIST_train_loader), loss.item()))\n",
        "        predicted_test_labels,gt_labels_tensor = validation_for_tuning(LogisticRegression,MNIST_validation_loader)\n",
        "        predicted_test_labels = predicted_test_labels.view(1200,10).cpu()\n",
        "        gt_labels_tensor = gt_labels_tensor.view(1200,10).cpu() \n",
        "    return predicted_test_labels , gt_labels_tensor"
      ],
      "metadata": {
        "id": "GxoHtIF6IX_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_hyper_parameter()"
      ],
      "metadata": {
        "id": "RIDKs1kFkb3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNgL7C7cQq-"
      },
      "source": [
        "Main loop. Run time and total score will be shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf9iL8S_cQrB",
        "outputId": "6b068eda-beab-47b1-cb1a-9a4cd7d10cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.598505\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.411011\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10854/12000 (90%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.434182\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.383970\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.252775\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.265898\n",
            "Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.313446\n",
            "Train Epoch: 3 [27000/48000 (56%)]\tLoss: 0.306595\n",
            "Train Epoch: 4 [0/48000 (0%)]\tLoss: 0.307379\n",
            "Train Epoch: 4 [27000/48000 (56%)]\tLoss: 0.341706\n",
            "Train Epoch: 5 [0/48000 (0%)]\tLoss: 0.358307\n",
            "Train Epoch: 5 [27000/48000 (56%)]\tLoss: 0.246384\n",
            "\n",
            "Validation set: Avg. loss: 0.0011, Accuracy: 11066/12000 (92%)\n",
            "\n",
            "Train Epoch: 6 [0/48000 (0%)]\tLoss: 0.264914\n",
            "Train Epoch: 6 [27000/48000 (56%)]\tLoss: 0.329686\n",
            "Train Epoch: 7 [0/48000 (0%)]\tLoss: 0.349647\n",
            "Train Epoch: 7 [27000/48000 (56%)]\tLoss: 0.270160\n",
            "Train Epoch: 8 [0/48000 (0%)]\tLoss: 0.331001\n",
            "Train Epoch: 8 [27000/48000 (56%)]\tLoss: 0.224204\n",
            "Train Epoch: 9 [0/48000 (0%)]\tLoss: 0.233979\n",
            "Train Epoch: 9 [27000/48000 (56%)]\tLoss: 0.345514\n",
            "Train Epoch: 10 [0/48000 (0%)]\tLoss: 0.170901\n",
            "Train Epoch: 10 [27000/48000 (56%)]\tLoss: 0.264830\n",
            "\n",
            "Validation set: Avg. loss: 0.0011, Accuracy: 11070/12000 (92%)\n",
            "\n",
            "Train Epoch: 11 [0/48000 (0%)]\tLoss: 0.356253\n",
            "Train Epoch: 11 [27000/48000 (56%)]\tLoss: 0.311112\n",
            "Train Epoch: 12 [0/48000 (0%)]\tLoss: 0.327808\n",
            "Train Epoch: 12 [27000/48000 (56%)]\tLoss: 0.319362\n",
            "Train Epoch: 13 [0/48000 (0%)]\tLoss: 0.271454\n",
            "Train Epoch: 13 [27000/48000 (56%)]\tLoss: 0.328013\n",
            "Train Epoch: 14 [0/48000 (0%)]\tLoss: 0.250218\n",
            "Train Epoch: 14 [27000/48000 (56%)]\tLoss: 0.406643\n",
            "\n",
            "Test set: Avg. loss: 0.0003, Accuracy: 9247/10000 (92%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 92 %\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.333533\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.913445\n",
            "\n",
            "Validation set: Avg. loss: 0.0093, Accuracy: 4351/12000 (36%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.844610\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.965579\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.869411\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.749048\n",
            "Train Epoch: 3 [0/38000 (0%)]\tLoss: 1.765397\n",
            "Train Epoch: 3 [20000/38000 (53%)]\tLoss: 1.860595\n",
            "Train Epoch: 4 [0/38000 (0%)]\tLoss: 1.706003\n",
            "Train Epoch: 4 [20000/38000 (53%)]\tLoss: 1.697918\n",
            "Train Epoch: 5 [0/38000 (0%)]\tLoss: 1.882459\n",
            "Train Epoch: 5 [20000/38000 (53%)]\tLoss: 1.655647\n",
            "\n",
            "Validation set: Avg. loss: 0.0088, Accuracy: 4801/12000 (40%)\n",
            "\n",
            "Train Epoch: 6 [0/38000 (0%)]\tLoss: 1.778902\n",
            "Train Epoch: 6 [20000/38000 (53%)]\tLoss: 1.754501\n",
            "Train Epoch: 7 [0/38000 (0%)]\tLoss: 1.767863\n",
            "Train Epoch: 7 [20000/38000 (53%)]\tLoss: 1.749010\n",
            "Train Epoch: 8 [0/38000 (0%)]\tLoss: 1.637605\n",
            "Train Epoch: 8 [20000/38000 (53%)]\tLoss: 1.772953\n",
            "Train Epoch: 9 [0/38000 (0%)]\tLoss: 1.694145\n",
            "Train Epoch: 9 [20000/38000 (53%)]\tLoss: 1.718270\n",
            "Train Epoch: 10 [0/38000 (0%)]\tLoss: 1.710260\n",
            "Train Epoch: 10 [20000/38000 (53%)]\tLoss: 1.729887\n",
            "\n",
            "Validation set: Avg. loss: 0.0087, Accuracy: 4896/12000 (41%)\n",
            "\n",
            "Train Epoch: 11 [0/38000 (0%)]\tLoss: 1.727072\n",
            "Train Epoch: 11 [20000/38000 (53%)]\tLoss: 1.573250\n",
            "Train Epoch: 12 [0/38000 (0%)]\tLoss: 1.725884\n",
            "Train Epoch: 12 [20000/38000 (53%)]\tLoss: 1.875356\n",
            "Train Epoch: 13 [0/38000 (0%)]\tLoss: 1.722424\n",
            "Train Epoch: 13 [20000/38000 (53%)]\tLoss: 1.685210\n",
            "Train Epoch: 14 [0/38000 (0%)]\tLoss: 1.565158\n",
            "Train Epoch: 14 [20000/38000 (53%)]\tLoss: 1.632712\n",
            "\n",
            "Test set: Avg. loss: 0.0017, Accuracy: 4071/10000 (41%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 40 %\n",
            "\n",
            "Result:\n",
            " OrderedDict([   (   'MNIST',\n",
            "                    OrderedDict([   ('correct_predict', 9247),\n",
            "                                    ('accuracy', 0.9247),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 118.33627447500021)])),\n",
            "                (   'CIFAR10',\n",
            "                    OrderedDict([   ('correct_predict', 4071),\n",
            "                                    ('accuracy', 0.4071),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 116.93438257399976)])),\n",
            "                ('total_score', 100.0)])\n"
          ]
        }
      ],
      "source": [
        "def run_on_dataset(dataset_name, filename):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        min_thres = 0.82\n",
        "        max_thres = 0.92\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        min_thres = 0.28\n",
        "        max_thres = 0.38\n",
        "\n",
        "    correct_predict, accuracy, run_time = run(logistic_regression, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                         accuracy=accuracy, score=score,\n",
        "                         run_time=run_time)\n",
        "    return result, score\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    filenames = { \"MNIST\": \"predictions_mnist_KhalidAlmahrezi_1580848.txt\", \"CIFAR10\": \"predictions_cifar10_KhalidAlmahrezi_1580848.txt\"}\n",
        "    result_all = OrderedDict()\n",
        "    score_weights = [0.5, 0.5]\n",
        "    scores = []\n",
        "    global learning_rate_MNIST\n",
        "    global Lambda_MNIST\n",
        "    global learning_rate_CIFAR10\n",
        "    global Lambda_CIFAR10\n",
        "    # final_best_best_params, best_accuracy, run_time = tune_hyper_parameter()\n",
        "    # final_best_best_params = {'MNIST': {'Adam': {'lr': 0.00149, 'lambda': 0.0051, 'best_accuracy': 0.9205833333333333}, 'SGD': {'lr': 0.00243, 'lambda': 0.00605, 'best_accuracy': 0.91575}}, 'CIFAR10': {'Adam': {'lr': 0.0009, 'lambda': 0.00343, 'best_accuracy': 0.3978}, 'SGD': {'lr': 0.0009, 'lambda': 0.00343, 'best_accuracy': 0.3972}}}\n",
        "    final_best_best_params = {\n",
        "                      \"MNIST\":{\"Adam\":{\"lr\": 0.0012, \"lambda\": 0.001,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0001, \"lambda\": 0.001,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":\"Adam\"},\n",
        "                              \n",
        "                    \"CIFAR10\":{\"Adam\":{\"lr\": 0.0001, \"lambda\": 0.0001,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0001, \"lambda\": 0.0001,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":\"Adam\"}}\n",
        "    global optimizerName_CIFAR10 \n",
        "    global optimizerName_MNIST \n",
        "    optimizerName_CIFAR10 = final_best_best_params[\"CIFAR10\"][\"best_optimizer\"]\n",
        "    optimizerName_MNIST= final_best_best_params[\"MNIST\"][\"best_optimizer\"]\n",
        "\n",
        "    learning_rate_CIFAR10 = final_best_best_params['CIFAR10'][optimizerName_CIFAR10]['lr']\n",
        "    Lambda_CIFAR10 = final_best_best_params['CIFAR10'][optimizerName_CIFAR10]['lambda']\n",
        "\n",
        "    learning_rate_MNIST = final_best_best_params['MNIST'][optimizerName_MNIST]['lr']\n",
        "    Lambda_MNIST = final_best_best_params['MNIST'][optimizerName_MNIST]['lambda']\n",
        "\n",
        "\n",
        "    for dataset_name in [\"MNIST\",\"CIFAR10\"]:\n",
        "    # for dataset_name in [\"CIFAR10\", \"MNIST\"]:\n",
        "        result_all[dataset_name], this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n",
        "        scores.append(this_score)\n",
        "    total_score = [score * weight for score, weight in zip(scores, score_weights)]\n",
        "    total_score = np.asarray(total_score).sum().item()\n",
        "    result_all['total_score'] = total_score\n",
        "    with open('result.txt', 'w') as f:\n",
        "        f.writelines(pformat(result_all, indent=4))\n",
        "    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "mGYSq1SfVeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d8nFkvRI4j3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a70d31-b804-4431-f89e-0e748d84a234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}