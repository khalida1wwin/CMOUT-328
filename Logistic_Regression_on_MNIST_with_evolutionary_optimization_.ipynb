{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalida1wwin/CMPUT-328/blob/main/Logistic_Regression_on_MNIST_with_evolutionary_optimization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVDq4R4cQqN"
      },
      "source": [
        "NOTE: DO NOT SUBMIT THIS NOTEBOOK FOR YOUR SUBMISSION!!!\n",
        "PLEASE SUBMIT \"A3_submission.py\" after you have finished debugging.\n",
        "\n",
        "Import and setup some auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHPwL1QYcQqU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "def compute_score(acc, min_thres, max_thres):\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return base_score\n",
        "\n",
        "\n",
        "def run(algorithm, dataset_name, filename):\n",
        "    start = timeit.default_timer()\n",
        "    predicted_test_labels, gt_labels = algorithm(dataset_name)\n",
        "    if predicted_test_labels is None or gt_labels is None:\n",
        "      return (0, 0, 0)\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - start\n",
        "    \n",
        "    np.savetxt(filename, np.asarray(predicted_test_labels))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for label, prediction in zip(gt_labels, predicted_test_labels):\n",
        "      total += label.size(0)\n",
        "      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n",
        "      \n",
        "    accuracy = float(correct) / total\n",
        "    \n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    return (correct, accuracy, run_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3VfLcA4cQqx"
      },
      "source": [
        "TODO: Implement Logistic Regression here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "# from CIFAR10_Multiple_Linear_Regression.ipynb on eclass\n",
        "CIFAR10_batch_size_train = 200\n",
        "MNIST_batch_size_train = 270\n",
        "batch_size_test = 1000\n",
        "from torch.utils.data import random_split\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "CIFAR10_training = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# create a training and a validation set\n",
        "CIFAR10_training_set, CIFAR10_validation_set = random_split(CIFAR10_training, [38000, 12000])\n",
        "\n",
        "# CIFAR-10 test set\n",
        "CIFAR10_test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "CIFAR10_train_loader = torch.utils.data.DataLoader(CIFAR10_training_set,\n",
        "                                           batch_size=CIFAR10_batch_size_train,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "CIFAR10_validation_loader = torch.utils.data.DataLoader(CIFAR10_validation_set,\n",
        "                                                batch_size=CIFAR10_batch_size_train,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "CIFAR10_test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,\n",
        "                                          batch_size=batch_size_test, \n",
        "                                          shuffle=False, num_workers=2)\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "#MNIST dataset is part of torchvision\n",
        "\n",
        "MNIST_training = datasets.MNIST('/MNIST_dataset/', train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "MNIST_test_set = datasets.MNIST('/MNIST_dataset/', train=False, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "# create a training and a validation set\n",
        "MNIST_training_set, MNIST_validation_set = random_split(MNIST_training, [48000, 12000])\n",
        "\n",
        "\n",
        "MNIST_train_loader = torch.utils.data.DataLoader(MNIST_training_set,batch_size=MNIST_batch_size_train, shuffle=True)\n",
        "\n",
        "MNIST_validation_loader = torch.utils.data.DataLoader(MNIST_validation_set,batch_size=MNIST_batch_size_train, shuffle=True)\n",
        "\n",
        "MNIST_test_loader = torch.utils.data.DataLoader(MNIST_test_set,batch_size=batch_size_test, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqcKTq63-JG3",
        "outputId": "943321b9-116e-4dab-b305-b8d1669ab1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MNIST_test_set.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nXk7iF1kqTG",
        "outputId": "34795d8a-ff9b-4fbf-a759-9863b6dae665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from CIFAR10_Multiple_Linear_Regression.ipynb on eclass\n",
        "# from torch.utils.data import random_split\n",
        "\n",
        "# momentum = 0.5\n",
        "log_interval = 100\n",
        "CIFAR10_n_epochs = int(3000 /(38000/CIFAR10_batch_size_train))\n",
        "MNIST_n_epochs = int(3800 /(48000/CIFAR10_batch_size_train))\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Checking GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(CIFAR10_n_epochs)\n",
        "print(MNIST_n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFgpwL3s-iNh",
        "outputId": "67878770-06f2-40ba-ac9d-d821e52ff629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "15\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(CIFAR10_test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)\n",
        "print(example_targets.shape)\n",
        "\n",
        "examples = enumerate(MNIST_test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)\n",
        "print(example_targets.shape)\n",
        "# print(example_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIkGSuF7_BAW",
        "outputId": "b6877303-5e2f-43e3-e881-ab45bcca0294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 3, 32, 32])\n",
            "torch.Size([1000])\n",
            "torch.Size([1000, 1, 28, 28])\n",
            "torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "MAnimj1Ka4OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple Linear regression\n",
        "class CIFAR10LogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(3*32*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Softmax = nn.Softmax(dim=1)\n",
        "        # x = Softmax(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "class MNISTLogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTLogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(1*28*28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Softmax = nn.Softmax(dim=1)\n",
        "        # x = Softmax(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zfe1sffIayqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class One_Hot(nn.Module):\n",
        "    def __init__(self, depth):\n",
        "        super(One_Hot,self).__init__()\n",
        "        self.depth = depth\n",
        "        self.ones = torch.sparse.torch.eye(depth).to(device)\n",
        "    def forward(self, X_in):\n",
        "        X_in = X_in.long()\n",
        "        return self.ones.index_select(0,X_in.data)\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"({})\".format(self.depth)"
      ],
      "metadata": {
        "id": "gtiY-xidfb9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(multi_linear_model,validation_loader):\n",
        "  multi_linear_model.eval()\n",
        "  validation_loss = 0\n",
        "  correct = 0\n",
        "  one_hot = One_Hot(10).to(device)\n",
        "  with torch.no_grad(): # notice the use of no_grad\n",
        "    for data, target in validation_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = multi_linear_model(data)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      ####\n",
        "      CE = nn.CrossEntropyLoss()\n",
        "      loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "      validation_loss +=loss\n",
        "      ####\n",
        "      # validation_loss += F.mse_loss(output, one_hot(target), size_average=False).item()\n",
        "\n",
        "  validation_loss /= len(validation_loader.dataset)\n",
        "  Accuracy = 100. * correct / len(validation_loader.dataset)\n",
        "  print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(validation_loss, correct, len(validation_loader.dataset), 100. * correct / len(validation_loader.dataset)))"
      ],
      "metadata": {
        "id": "TO667fjjEcaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(multi_linear_model,test_loader):\n",
        "  multi_linear_model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  one_hot = One_Hot(10).to(device)\n",
        "  # gt_labels_tensor = torch.zeros(len(test_loader),dtype=torch.float,device= device)\n",
        "  final_target = torch.FloatTensor([]).to(device)\n",
        "  final_pred = torch.FloatTensor([]).to(device)\n",
        "  with torch.no_grad():\n",
        "    for i, (data, target) in enumerate(test_loader):\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      final_target = torch.cat([final_target, target]).view(-1).to(device)\n",
        "      \n",
        "      # print(target)\n",
        "      # print(target.shape)\n",
        "      # gt_labels_tensor[i] = target\n",
        "      \n",
        "      output = multi_linear_model(data)\n",
        "      ####\n",
        "      CE = nn.CrossEntropyLoss()\n",
        "      loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "      test_loss +=loss\n",
        "      ####\n",
        "      # test_loss += F.mse_loss(output, one_hot(target), size_average=False).item()\n",
        "      \n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      # print(pred)\n",
        "      final_pred = torch.cat([final_pred, pred.view_as(target)])\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "      \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "  # print(final_pred.shape, final_target.shape)\n",
        "  # print(final_pred, final_target)\n",
        "  return final_pred, final_target"
      ],
      "metadata": {
        "id": "i0WSdITjRobF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Mjmw05cQq0"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(dataset_name):\n",
        "    # epoch = 5\n",
        "    log_interval = 100\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if dataset_name == \"CIFAR10\":\n",
        "      # Lambda = 0.001\n",
        "      # learning_rate = 0.0005\n",
        "      Lambda = Lambda_CIFAR10\n",
        "      learning_rate = learning_rate_CIFAR10\n",
        "      LogisticRegression = CIFAR10LogisticRegression().to(device)\n",
        "      # optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95) # the best\n",
        "      # optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      if optimizerName_CIFAR10 == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName_CIFAR10 == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      # validation(LogisticRegression,CIFAR10_validation_loader)\n",
        "      for epoch in range(CIFAR10_n_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(CIFAR10_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # validation(LogisticRegression,CIFAR10_validation_loader)\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(CIFAR10_train_loader.dataset),\n",
        "              100. * batch_idx / len(CIFAR10_train_loader), loss.item()))\n",
        "        if epoch % 5 == 0:\n",
        "          validation(LogisticRegression,CIFAR10_validation_loader)\n",
        "      # test after training \n",
        "      predicted_test_labels , gt_labels_tensor = test(LogisticRegression,CIFAR10_test_loader)\n",
        "      gt_labels = CIFAR10_test_set.targets\n",
        "    elif dataset_name == \"MNIST\":\n",
        "      # Lambda = 0.0001\n",
        "      # learning_rate = 0.001\n",
        "      Lambda = Lambda_MNIST\n",
        "      learning_rate = learning_rate_MNIST\n",
        "      LogisticRegression = MNISTLogisticRegression().to(device)\n",
        "      # optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      # optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate) # The best \n",
        "      if optimizerName_MNIST == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName_MNIST == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      # validation(LogisticRegression,MNIST_validation_loader)\n",
        "      for epoch in range(MNIST_n_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(MNIST_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # validation(LogisticRegression,MNIST_validation_loader)\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(MNIST_train_loader.dataset),\n",
        "              100. * batch_idx / len(MNIST_train_loader), loss.item()))\n",
        "        if epoch % 5 == 0:\n",
        "          validation(LogisticRegression,MNIST_validation_loader)\n",
        "      # test after training \n",
        "      predicted_test_labels,gt_labels_tensor = test(LogisticRegression,MNIST_test_loader)\n",
        "    return predicted_test_labels.view(1000,10).cpu() , gt_labels_tensor.view(1000,10).cpu() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzZl82Ly1RFw"
      },
      "source": [
        "TODO: Implement Hyper-parameter Tuning here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfDNwazz1RFw"
      },
      "outputs": [],
      "source": [
        "def tune_hyper_parameter():\n",
        "    # TODO: implement logistic regression hyper-parameter tuning here\n",
        "    # learning rate and lambda\n",
        "    startTime = timeit.default_timer()\n",
        "    params_to_tune = [{\"lr\": 0.01 , \"lambda\": 0.001}, \n",
        "                      {\"lr\": 0.01 , \"lambda\": 0.0001},\n",
        "                      {\"lr\": 0.001, \"lambda\": 0.001}, \n",
        "                      {\"lr\": 0.001, \"lambda\": 0.0001},\n",
        "                      {\"lr\": 0.01 , \"lambda\": 0.01},\n",
        "                      {\"lr\": 0.0005, \"lambda\": 0.01}, \n",
        "                      {\"lr\": 0.0005, \"lambda\": 0.001}]\n",
        "    \n",
        "    best_accuracy = 0.0\n",
        "    best_params = None\n",
        "    filenames = { \"MNIST\": \"predictions_mnist_KhalidAlmahrezi_1580848.txt\", \"CIFAR10\": \"predictions_cifar10_KhalidAlmahrezi_1580848.txt\"}\n",
        "\n",
        "    final_best_best_params = {\"Adam\":{\"lr\":0.0 , \"lambda\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0, \"lambda\": 0.0}}\n",
        "    global learning_rate \n",
        "    global Lambda \n",
        "    global optimizerName \n",
        "    for dataset_name in [\"CIFAR10\", \"MNIST\"]:\n",
        "    # for dataset_name in [ \"MNIST\", \"CIFAR10\"]:\n",
        "      for optimizerName in [\"Adam\",\"SGD\"]:\n",
        "        for params in params_to_tune:\n",
        "          learning_rate = params[\"lr\"]\n",
        "          Lambda = params[\"lambda\"]\n",
        "          print(\"optimizer Name: \", optimizerName, \"| learning rate = \", learning_rate, \"| Lambda = \", Lambda)\n",
        "        # global params    # Specify params to search as a global variable, to be used for logistic_regression, also feel free to add more arguments to all existing functions\n",
        "          result, score = run_on_dataset_for_tuning(dataset_name, filenames[dataset_name])\n",
        "          if result[\"accuracy\"] > best_accuracy:\n",
        "              best_accuracy = result[\"accuracy\"]\n",
        "              best_params = params\n",
        "              final_best_best_params[optimizerName][\"lr\"] = learning_rate\n",
        "              final_best_best_params[optimizerName][\"lambda\"] = Lambda\n",
        "          print(best_params, best_accuracy)\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - startTime\n",
        "    print(best_params, best_accuracy, run_time)\n",
        "    print(final_best_best_params, best_accuracy, run_time)\n",
        "\n",
        "    return final_best_best_params, best_accuracy, run_time\n",
        "    \n",
        "    # return None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import heapq\n",
        "# rng_seed = np.random.default_rng(143341)\n",
        "# LR = (rng_seed.integers(low=0, high=1000, size=3)/10000) \n",
        "# Lammda =  (rng_seed.integers(low=0, high=1000, size=3)/100000) \n",
        "\n",
        "# LR_2d = []\n",
        "# Lammda_2d = []\n",
        "# for i in range(len(LR)):\n",
        "#   heapq.heappush(LR_2d,([i,LR[i]]))\n",
        "#   heapq.heappush(Lammda_2d,([i,Lammda[i]]))\n",
        "# # for i in [0.91, 0.82, 0.89]:\n",
        "\n",
        "\n",
        "# print(\"lammda\",Lammda_2d,\"LR\",LR_2d)\n",
        "# print(Lammda_2d,LR_2d)\n",
        "# print(heapq.heappop(Lammda_2d))\n",
        "# print(heapq.heappop(LR_2d))\n",
        "# print(\"lammda\",Lammda_2d,\"LR\",LR_2d)\n",
        "# print((rng_seed.integers(low=0, high=1000, size=1)/10000)[0])\n",
        "# print(LR_2d)\n",
        "# print(Lammda_2d)\n",
        "# sorted(Lammda_2d)\n",
        "# # heapq.heapify([el * -1 for el in LR_2d ])\n",
        "\n",
        "# print(LR_2d)\n",
        "# print(Lammda_2d)\n",
        "# print(LR_2d.pop())\n",
        "# print(Lammda_2d.pop())\n",
        "# print(LR_2d)\n",
        "# print(Lammda_2d)"
      ],
      "metadata": {
        "id": "7OWiMwhCxTqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def newRandomList(From, To, Size):\n",
        "\n",
        "\n",
        "      \n",
        "#     return newList"
      ],
      "metadata": {
        "id": "diqRNuIcAOT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyper_parameter():\n",
        "    # TODO: implement logistic regression hyper-parameter tuning here\n",
        "    # learning rate and lambda\n",
        "    # with Evolutionary optimization from\n",
        "    # https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization\n",
        "    startTime = timeit.default_timer()\n",
        "    import numpy as np\n",
        "    import heapq\n",
        "    \n",
        "    div = 1000000\n",
        "    GenSize = 3\n",
        "    remove = 2\n",
        "    best_accuracy = 0.0\n",
        "    genarations = 4\n",
        "    \n",
        "\n",
        "    \n",
        "    final_best_best_params = {\n",
        "                      \"MNIST\":{\"Adam\":{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":None},\n",
        "                              \n",
        "                    \"CIFAR10\":{\"Adam\":{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0, \"lambda\": 0.0,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":None}}\n",
        "    best_params = None\n",
        "    filenames = { \"MNIST\": \"predictions_mnist_KhalidAlmahrezi_1580848.txt\", \"CIFAR10\": \"predictions_cifar10_KhalidAlmahrezi_1580848.txt\"}\n",
        "    global learning_rate \n",
        "    global Lambda \n",
        "    global optimizerName\n",
        "    global epochs  \n",
        "    # for dataset_name in [\"CIFAR10\", \"MNIST\"]:\n",
        "    for dataset_name in [ \"MNIST\", \"CIFAR10\"]:\n",
        "      for optimizerName in [\"Adam\",\"SGD\"]:\n",
        "        # 1 Create an initial population of random solutions\n",
        "        rng_seed = np.random.default_rng(143341)\n",
        "        \n",
        "        rng_seed = np.random.default_rng(143341)\n",
        "        learning_rateGen1 = rng_seed.integers(low=0, high=10000, size=GenSize)/div\n",
        "        LammdaGen1 =  rng_seed.integers(low=0, high=10000, size=GenSize)/div\n",
        "        LR_2d = []\n",
        "        Lammda_2d = []\n",
        "        print(\"learning_rateGen1\")\n",
        "        print(learning_rateGen1)\n",
        "        print(\"LammdaGen1\")\n",
        "        print(LammdaGen1)\n",
        "        for i in range(GenSize):\n",
        "          heapq.heappush(LR_2d,([0,learning_rateGen1[i]]))\n",
        "          heapq.heappush(Lammda_2d,([0,LammdaGen1[i]]))\n",
        "\n",
        "        for gen in range(genarations):\n",
        "          print(\"gen:\", gen)\n",
        "          newLR_2d = []\n",
        "          newLammda_2d = []\n",
        "          # start of the generations\n",
        "          for i in range(GenSize):\n",
        "            # 2 Evaluate the hyperparameters tuples and acquire their fitness function\n",
        "            learning_rate = LR_2d[i][1]\n",
        "            Lambda = Lammda_2d[i][1]\n",
        "            print(\"optimizer Name: \", optimizerName, \"| learning rate = \", learning_rate, \"| Lambda = \", Lambda)\n",
        "            if gen == 0: \n",
        "              epochs = 1\n",
        "            elif gen == 3: \n",
        "              epochs = 3\n",
        "            # elif gen == 5:\n",
        "            #   epochs = 4\n",
        "            # elif gen == 6:\n",
        "            #   epochs = 6\n",
        "          #  global params    # Specify params to search as a global variable, to be used for logistic_regression, also feel free to add more arguments to all existing functions\n",
        "            result, score = run_on_dataset_for_tuning(dataset_name, filenames[dataset_name])\n",
        "            if result[\"accuracy\"] > final_best_best_params[dataset_name][optimizerName][\"best_accuracy\"]:\n",
        "              best_accuracy = result[\"accuracy\"]\n",
        "              final_best_best_params[dataset_name][optimizerName][\"best_accuracy\"] =  result[\"accuracy\"]\n",
        "              final_best_best_params[dataset_name][\"best_optimizer\"] =  optimizerName\n",
        "              best_params = {learning_rate,Lambda}\n",
        "              # 3 Rank the hyperparameter tuples by their relative fitness\n",
        "              heapq.heappush(newLR_2d,[result[\"accuracy\"],learning_rate])\n",
        "              heapq.heappush(newLammda_2d,[result[\"accuracy\"],Lambda])\n",
        "              final_best_best_params[dataset_name][optimizerName][\"lr\"] = learning_rate\n",
        "              final_best_best_params[dataset_name][optimizerName][\"lambda\"] = Lambda \n",
        "              # print(best_params, best_accuracy)\n",
        "            else:\n",
        "              heapq.heappush(newLR_2d,[result[\"accuracy\"],learning_rate])\n",
        "              heapq.heappush(newLammda_2d,[result[\"accuracy\"],Lambda])\n",
        "\n",
        "\n",
        "            # 4 Replace the worst-performing hyperparameter tuples with new hyperparameter tuples generated\n",
        "            print(\"newLR_2d\", newLR_2d,\"newLammda_2d\",newLammda_2d,)\n",
        "\n",
        "          # select the best two acc two replace the worst acc\n",
        "          for n in range(remove):\n",
        "            sorted_LR_2d = sorted(newLR_2d.copy())\n",
        "            sortedLimts_LR_2d = []\n",
        "            heapq.heappush(sortedLimts_LR_2d, sorted_LR_2d.pop()[1])\n",
        "            heapq.heappush(sortedLimts_LR_2d, sorted_LR_2d.pop()[1])\n",
        "            High_LR_2d = sortedLimts_LR_2d[1]\n",
        "            Low_LR_2d = sortedLimts_LR_2d[0]\n",
        "\n",
        "\n",
        "\n",
        "            sorted_Lammda_2d = sorted(newLammda_2d.copy())\n",
        "            sortedLimts_Lammda_2d = []\n",
        "            heapq.heappush(sortedLimts_Lammda_2d, sorted_Lammda_2d.pop()[1])\n",
        "            heapq.heappush(sortedLimts_Lammda_2d, sorted_Lammda_2d.pop()[1])\n",
        "            High_Lammda_2d = sortedLimts_Lammda_2d[1]\n",
        "            Low_Lammda_2d = sortedLimts_Lammda_2d[0]\n",
        "            # print(sortedlimts_LR_2d)\n",
        "            print(\"low= \",Low_LR_2d, \"high=\",High_LR_2d)\n",
        "            # print(sortedlimts_Lammda_2d)\n",
        "            print(\"low= \",Low_Lammda_2d, \"high=\",High_Lammda_2d)\n",
        "            heapq.heappop(newLR_2d)\n",
        "            heapq.heappop(newLammda_2d)\n",
        "            if High_LR_2d != Low_LR_2d:\n",
        "              heapq.heappush(newLR_2d, [0, (rng_seed.integers(low=Low_LR_2d* div, high=High_LR_2d* div, size=1)/div)[0]])\n",
        "            else:\n",
        "              heapq.heappush(newLR_2d, [0, High_LR_2d/2])\n",
        "            \n",
        "            if High_Lammda_2d != Low_Lammda_2d:  \n",
        "              heapq.heappush(newLammda_2d, [0, (rng_seed.integers(low=Low_Lammda_2d * div, high=High_Lammda_2d* div, size=1)/div)[0]])\n",
        "            else:\n",
        "              heapq.heappush(newLammda_2d, [0, High_Lammda_2d/2])\n",
        "            # update the new gen list\n",
        "            # print(\"befor heapify\",newLR_2d,newLammda_2d)\n",
        "            # heapq.heapify(newLR_2d)\n",
        "            # heapq.heapify(newLammda_2d)\n",
        "            # print(\"after heapify\",newLR_2d,newLammda_2d)\n",
        "            LR_2d = newLR_2d\n",
        "            Lammda_2d = newLammda_2d\n",
        "          print(\"LR_2d\", LR_2d,\"Lammda_2d\",Lammda_2d,)\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - startTime\n",
        "    print(final_best_best_params, best_accuracy, run_time)\n",
        "    return final_best_best_params, best_accuracy, run_time\n"
      ],
      "metadata": {
        "id": "nYBsROwupNqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_on_dataset_for_tuning(dataset_name, filename):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        min_thres = 0.82\n",
        "        max_thres = 0.92\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        min_thres = 0.28\n",
        "        max_thres = 0.38\n",
        "\n",
        "    correct_predict, accuracy, run_time = run(logistic_regression_for_tuning, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                         accuracy=accuracy, score=score,\n",
        "                         run_time=run_time)\n",
        "    return result, score"
      ],
      "metadata": {
        "id": "0EsLhrDAI2Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_for_tuning(multi_linear_model,validation_loader):\n",
        "  multi_linear_model.eval()\n",
        "  validation_loss = 0\n",
        "  correct = 0\n",
        "  one_hot = One_Hot(10).to(device)\n",
        "  final_target = torch.FloatTensor([]).to(device)\n",
        "  final_pred = torch.FloatTensor([]).to(device)\n",
        "  with torch.no_grad(): \n",
        "    for data, target in validation_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      target = target.to(device)\n",
        "      final_target = torch.cat([final_target, target]).view(-1).to(device)\n",
        "      output = multi_linear_model(data)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      final_pred = torch.cat([final_pred, pred.view_as(target)])\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      CE = nn.CrossEntropyLoss()\n",
        "      loss = CE(output, one_hot(target))\n",
        "      validation_loss +=loss\n",
        "      \n",
        "  validation_loss /= len(validation_loader.dataset)\n",
        "  Accuracy = 100. * correct / len(validation_loader.dataset)\n",
        "  print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(validation_loss, correct, len(validation_loader.dataset), 100. * correct / len(validation_loader.dataset)))\n",
        "\n",
        "  return final_pred, final_target"
      ],
      "metadata": {
        "id": "VnXxu2Xkz8Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_for_tuning(dataset_name):\n",
        "\n",
        "    print(\"In the logistic_regression_for_tuning\", \"optimizerName: \", optimizerName, \"| learning rate = \", learning_rate, \"| Lambda = \", Lambda)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if dataset_name == \"CIFAR10\":\n",
        "      LogisticRegression = CIFAR10LogisticRegression().to(device)\n",
        "      if optimizerName == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      for epoch in range(epochs):\n",
        "        for batch_idx, (data, target) in enumerate(CIFAR10_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) \n",
        "\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(CIFAR10_train_loader.dataset),\n",
        "              100. * batch_idx / len(CIFAR10_train_loader), loss.item()))\n",
        "        predicted_test_labels,gt_labels_tensor = validation_for_tuning(LogisticRegression,CIFAR10_test_loader)\n",
        "        predicted_test_labels = predicted_test_labels.view(1000,10).cpu()\n",
        "        gt_labels_tensor = gt_labels_tensor.view(1000,10).cpu() \n",
        "    elif dataset_name == \"MNIST\":\n",
        "      LogisticRegression = MNISTLogisticRegression().to(device)\n",
        "      if optimizerName == \"SGD\":\n",
        "        optimizer = optim.SGD(LogisticRegression.parameters(), lr=learning_rate, momentum=0.95)\n",
        "      elif optimizerName == \"Adam\": \n",
        "        optimizer = optim.Adam(LogisticRegression.parameters(), lr=learning_rate)\n",
        "      one_hot = One_Hot(10).to(device)\n",
        "      LogisticRegression.train()\n",
        "      for epoch in range(epochs):\n",
        "        for batch_idx, (data, target) in enumerate(MNIST_train_loader):\n",
        "          data = data.requires_grad_().to(device)\n",
        "          target = target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = LogisticRegression(data)\n",
        "          CE = nn.CrossEntropyLoss()\n",
        "          loss = CE(output, one_hot(target)) # notice the use of view_as\n",
        "          # L = [(torch.abs(p)).sum() for p in LogisticRegression.parameters()] #L1\n",
        "          L = [(p**2).sum() for p in LogisticRegression.parameters()] #L2\n",
        "          loss = loss + Lambda * sum(L)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(MNIST_train_loader.dataset),\n",
        "              100. * batch_idx / len(MNIST_train_loader), loss.item()))\n",
        "        predicted_test_labels,gt_labels_tensor = validation_for_tuning(LogisticRegression,MNIST_validation_loader)\n",
        "        predicted_test_labels = predicted_test_labels.view(1200,10).cpu()\n",
        "        gt_labels_tensor = gt_labels_tensor.view(1200,10).cpu() \n",
        "    return predicted_test_labels , gt_labels_tensor"
      ],
      "metadata": {
        "id": "GxoHtIF6IX_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_hyper_parameter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RIDKs1kFkb3a",
        "outputId": "9ff31bc1-30d0-434b-da77-3ce7f6efcdd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rateGen1\n",
            "[0.004907 0.008722 0.000907]\n",
            "LammdaGen1\n",
            "[0.004911 0.006292 0.009938]\n",
            "gen: 0\n",
            "optimizer Name:  Adam | learning rate =  0.000907 | Lambda =  0.004911\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.004911\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.529174\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.466317\n",
            "\n",
            "Validation set: Avg. loss: 0.0014, Accuracy: 10796/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "newLR_2d [[0.8996666666666666, 0.000907]] newLammda_2d [[0.8996666666666666, 0.004911]]\n",
            "optimizer Name:  Adam | learning rate =  0.008722 | Lambda =  0.006292\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.008722 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.478882\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.513249\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10758/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "newLR_2d [[0.8965, 0.008722], [0.8996666666666666, 0.000907]] newLammda_2d [[0.8965, 0.006292], [0.8996666666666666, 0.004911]]\n",
            "optimizer Name:  Adam | learning rate =  0.004907 | Lambda =  0.009938\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.531179\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.285338\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10764/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "newLR_2d [[0.8965, 0.008722], [0.8996666666666666, 0.000907], [0.897, 0.004907]] newLammda_2d [[0.8965, 0.006292], [0.8996666666666666, 0.004911], [0.897, 0.009938]]\n",
            "low=  0.000907 high= 0.004907\n",
            "low=  0.004911 high= 0.009938\n",
            "low=  0.000907 high= 0.004907\n",
            "low=  0.004911 high= 0.009938\n",
            "LR_2d [[0, 0.001497], [0.8996666666666666, 0.000907], [0.897, 0.004907]] Lammda_2d [[0, 0.007305], [0.8996666666666666, 0.004911], [0.897, 0.009938]]\n",
            "gen: 1\n",
            "optimizer Name:  Adam | learning rate =  0.001497 | Lambda =  0.007305\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.001497 | Lambda =  0.007305\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.622869\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.451310\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10857/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "newLR_2d [[0.90475, 0.001497]] newLammda_2d [[0.90475, 0.007305]]\n",
            "optimizer Name:  Adam | learning rate =  0.000907 | Lambda =  0.004911\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.004911\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.447152\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.460113\n",
            "\n",
            "Validation set: Avg. loss: 0.0014, Accuracy: 10785/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "newLR_2d [[0.89875, 0.000907], [0.90475, 0.001497]] newLammda_2d [[0.89875, 0.004911], [0.90475, 0.007305]]\n",
            "optimizer Name:  Adam | learning rate =  0.004907 | Lambda =  0.009938\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.652978\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.338431\n",
            "\n",
            "Validation set: Avg. loss: 0.0014, Accuracy: 10703/12000 (89%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "newLR_2d [[0.8919166666666667, 0.004907], [0.90475, 0.001497], [0.89875, 0.000907]] newLammda_2d [[0.8919166666666667, 0.009938], [0.90475, 0.007305], [0.89875, 0.004911]]\n",
            "low=  0.000907 high= 0.001497\n",
            "low=  0.004911 high= 0.007305\n",
            "low=  0.000907 high= 0.001497\n",
            "low=  0.004911 high= 0.007305\n",
            "LR_2d [[0, 0.001343], [0.90475, 0.001497], [0.89875, 0.000907]] Lammda_2d [[0, 0.007009], [0.90475, 0.007305], [0.89875, 0.004911]]\n",
            "gen: 2\n",
            "optimizer Name:  Adam | learning rate =  0.001343 | Lambda =  0.007009\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.001343 | Lambda =  0.007009\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.495799\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-8f5140afe824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtune_hyper_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-ce407c3239c9>\u001b[0m in \u001b[0;36mtune_hyper_parameter\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m#   epochs = 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0;31m#  global params    # Specify params to search as a global variable, to be used for logistic_regression, also feel free to add more arguments to all existing functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_on_dataset_for_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfinal_best_best_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizerName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"best_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m               \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-54efc720f4bf>\u001b[0m in \u001b[0;36mrun_on_dataset_for_tuning\u001b[0;34m(dataset_name, filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmax_thres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.38\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcorrect_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression_for_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_thres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d7f448a62509>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(algorithm, dataset_name, filename)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpredicted_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpredicted_test_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgt_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-0e9ebf8dd462>\u001b[0m in \u001b[0;36mlogistic_regression_for_tuning\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNIST_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m           \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2670\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2671\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNgL7C7cQq-"
      },
      "source": [
        "Main loop. Run time and total score will be shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf9iL8S_cQrB",
        "outputId": "6b068eda-beab-47b1-cb1a-9a4cd7d10cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.598505\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.411011\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10854/12000 (90%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.434182\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.383970\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.252775\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.265898\n",
            "Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.313446\n",
            "Train Epoch: 3 [27000/48000 (56%)]\tLoss: 0.306595\n",
            "Train Epoch: 4 [0/48000 (0%)]\tLoss: 0.307379\n",
            "Train Epoch: 4 [27000/48000 (56%)]\tLoss: 0.341706\n",
            "Train Epoch: 5 [0/48000 (0%)]\tLoss: 0.358307\n",
            "Train Epoch: 5 [27000/48000 (56%)]\tLoss: 0.246384\n",
            "\n",
            "Validation set: Avg. loss: 0.0011, Accuracy: 11066/12000 (92%)\n",
            "\n",
            "Train Epoch: 6 [0/48000 (0%)]\tLoss: 0.264914\n",
            "Train Epoch: 6 [27000/48000 (56%)]\tLoss: 0.329686\n",
            "Train Epoch: 7 [0/48000 (0%)]\tLoss: 0.349647\n",
            "Train Epoch: 7 [27000/48000 (56%)]\tLoss: 0.270160\n",
            "Train Epoch: 8 [0/48000 (0%)]\tLoss: 0.331001\n",
            "Train Epoch: 8 [27000/48000 (56%)]\tLoss: 0.224204\n",
            "Train Epoch: 9 [0/48000 (0%)]\tLoss: 0.233979\n",
            "Train Epoch: 9 [27000/48000 (56%)]\tLoss: 0.345514\n",
            "Train Epoch: 10 [0/48000 (0%)]\tLoss: 0.170901\n",
            "Train Epoch: 10 [27000/48000 (56%)]\tLoss: 0.264830\n",
            "\n",
            "Validation set: Avg. loss: 0.0011, Accuracy: 11070/12000 (92%)\n",
            "\n",
            "Train Epoch: 11 [0/48000 (0%)]\tLoss: 0.356253\n",
            "Train Epoch: 11 [27000/48000 (56%)]\tLoss: 0.311112\n",
            "Train Epoch: 12 [0/48000 (0%)]\tLoss: 0.327808\n",
            "Train Epoch: 12 [27000/48000 (56%)]\tLoss: 0.319362\n",
            "Train Epoch: 13 [0/48000 (0%)]\tLoss: 0.271454\n",
            "Train Epoch: 13 [27000/48000 (56%)]\tLoss: 0.328013\n",
            "Train Epoch: 14 [0/48000 (0%)]\tLoss: 0.250218\n",
            "Train Epoch: 14 [27000/48000 (56%)]\tLoss: 0.406643\n",
            "\n",
            "Test set: Avg. loss: 0.0003, Accuracy: 9247/10000 (92%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 92 %\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.333533\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.913445\n",
            "\n",
            "Validation set: Avg. loss: 0.0093, Accuracy: 4351/12000 (36%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.844610\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.965579\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.869411\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.749048\n",
            "Train Epoch: 3 [0/38000 (0%)]\tLoss: 1.765397\n",
            "Train Epoch: 3 [20000/38000 (53%)]\tLoss: 1.860595\n",
            "Train Epoch: 4 [0/38000 (0%)]\tLoss: 1.706003\n",
            "Train Epoch: 4 [20000/38000 (53%)]\tLoss: 1.697918\n",
            "Train Epoch: 5 [0/38000 (0%)]\tLoss: 1.882459\n",
            "Train Epoch: 5 [20000/38000 (53%)]\tLoss: 1.655647\n",
            "\n",
            "Validation set: Avg. loss: 0.0088, Accuracy: 4801/12000 (40%)\n",
            "\n",
            "Train Epoch: 6 [0/38000 (0%)]\tLoss: 1.778902\n",
            "Train Epoch: 6 [20000/38000 (53%)]\tLoss: 1.754501\n",
            "Train Epoch: 7 [0/38000 (0%)]\tLoss: 1.767863\n",
            "Train Epoch: 7 [20000/38000 (53%)]\tLoss: 1.749010\n",
            "Train Epoch: 8 [0/38000 (0%)]\tLoss: 1.637605\n",
            "Train Epoch: 8 [20000/38000 (53%)]\tLoss: 1.772953\n",
            "Train Epoch: 9 [0/38000 (0%)]\tLoss: 1.694145\n",
            "Train Epoch: 9 [20000/38000 (53%)]\tLoss: 1.718270\n",
            "Train Epoch: 10 [0/38000 (0%)]\tLoss: 1.710260\n",
            "Train Epoch: 10 [20000/38000 (53%)]\tLoss: 1.729887\n",
            "\n",
            "Validation set: Avg. loss: 0.0087, Accuracy: 4896/12000 (41%)\n",
            "\n",
            "Train Epoch: 11 [0/38000 (0%)]\tLoss: 1.727072\n",
            "Train Epoch: 11 [20000/38000 (53%)]\tLoss: 1.573250\n",
            "Train Epoch: 12 [0/38000 (0%)]\tLoss: 1.725884\n",
            "Train Epoch: 12 [20000/38000 (53%)]\tLoss: 1.875356\n",
            "Train Epoch: 13 [0/38000 (0%)]\tLoss: 1.722424\n",
            "Train Epoch: 13 [20000/38000 (53%)]\tLoss: 1.685210\n",
            "Train Epoch: 14 [0/38000 (0%)]\tLoss: 1.565158\n",
            "Train Epoch: 14 [20000/38000 (53%)]\tLoss: 1.632712\n",
            "\n",
            "Test set: Avg. loss: 0.0017, Accuracy: 4071/10000 (41%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 40 %\n",
            "\n",
            "Result:\n",
            " OrderedDict([   (   'MNIST',\n",
            "                    OrderedDict([   ('correct_predict', 9247),\n",
            "                                    ('accuracy', 0.9247),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 118.33627447500021)])),\n",
            "                (   'CIFAR10',\n",
            "                    OrderedDict([   ('correct_predict', 4071),\n",
            "                                    ('accuracy', 0.4071),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 116.93438257399976)])),\n",
            "                ('total_score', 100.0)])\n"
          ]
        }
      ],
      "source": [
        "def run_on_dataset(dataset_name, filename):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        min_thres = 0.82\n",
        "        max_thres = 0.92\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        min_thres = 0.28\n",
        "        max_thres = 0.38\n",
        "\n",
        "    correct_predict, accuracy, run_time = run(logistic_regression, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                         accuracy=accuracy, score=score,\n",
        "                         run_time=run_time)\n",
        "    return result, score\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    filenames = { \"MNIST\": \"predictions_mnist_KhalidAlmahrezi_1580848.txt\", \"CIFAR10\": \"predictions_cifar10_KhalidAlmahrezi_1580848.txt\"}\n",
        "    result_all = OrderedDict()\n",
        "    score_weights = [0.5, 0.5]\n",
        "    scores = []\n",
        "    global learning_rate_MNIST\n",
        "    global Lambda_MNIST\n",
        "    global learning_rate_CIFAR10\n",
        "    global Lambda_CIFAR10\n",
        "    # final_best_best_params, best_accuracy, run_time = tune_hyper_parameter()\n",
        "    # final_best_best_params = {'MNIST': {'Adam': {'lr': 0.00149, 'lambda': 0.0051, 'best_accuracy': 0.9205833333333333}, 'SGD': {'lr': 0.00243, 'lambda': 0.00605, 'best_accuracy': 0.91575}}, 'CIFAR10': {'Adam': {'lr': 0.0009, 'lambda': 0.00343, 'best_accuracy': 0.3978}, 'SGD': {'lr': 0.0009, 'lambda': 0.00343, 'best_accuracy': 0.3972}}}\n",
        "    final_best_best_params = {\n",
        "                      \"MNIST\":{\"Adam\":{\"lr\": 0.0012, \"lambda\": 0.001,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0001, \"lambda\": 0.001,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":\"Adam\"},\n",
        "                              \n",
        "                    \"CIFAR10\":{\"Adam\":{\"lr\": 0.0001, \"lambda\": 0.0001,\"best_accuracy\": 0.0}, \n",
        "                              \"SGD\" :{\"lr\": 0.0001, \"lambda\": 0.0001,\"best_accuracy\": 0.0},\n",
        "                              \"best_optimizer\":\"Adam\"}}\n",
        "    global optimizerName_CIFAR10 \n",
        "    global optimizerName_MNIST \n",
        "    optimizerName_CIFAR10 = final_best_best_params[\"CIFAR10\"][\"best_optimizer\"]\n",
        "    optimizerName_MNIST= final_best_best_params[\"MNIST\"][\"best_optimizer\"]\n",
        "\n",
        "    learning_rate_CIFAR10 = final_best_best_params['CIFAR10'][optimizerName_CIFAR10]['lr']\n",
        "    Lambda_CIFAR10 = final_best_best_params['CIFAR10'][optimizerName_CIFAR10]['lambda']\n",
        "\n",
        "    learning_rate_MNIST = final_best_best_params['MNIST'][optimizerName_MNIST]['lr']\n",
        "    Lambda_MNIST = final_best_best_params['MNIST'][optimizerName_MNIST]['lambda']\n",
        "\n",
        "\n",
        "    for dataset_name in [\"MNIST\",\"CIFAR10\"]:\n",
        "    # for dataset_name in [\"CIFAR10\", \"MNIST\"]:\n",
        "        result_all[dataset_name], this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n",
        "        scores.append(this_score)\n",
        "    total_score = [score * weight for score, weight in zip(scores, score_weights)]\n",
        "    total_score = np.asarray(total_score).sum().item()\n",
        "    result_all['total_score'] = total_score\n",
        "    with open('result.txt', 'w') as f:\n",
        "        f.writelines(pformat(result_all, indent=4))\n",
        "    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "mGYSq1SfVeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d8nFkvRI4j3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a70d31-b804-4431-f89e-0e748d84a234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"[ur path]/A3_submission.py\""
      ],
      "metadata": {
        "id": "BynlyVaDxv-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"/content/drive/MyDrive/5year/CMPUT 328/ASSIG/3/A3_submission.py\""
      ],
      "metadata": {
        "id": "Vq33leI-4yTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451f1b76-3080-44b9-f0f9-975406d26594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "part 1\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.297991\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.352753\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10820/12000 (90%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.338789\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.324663\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.393180\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.366150\n",
            "Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.256788\n",
            "Train Epoch: 3 [27000/48000 (56%)]\tLoss: 0.279263\n",
            "Train Epoch: 4 [0/48000 (0%)]\tLoss: 0.348849\n",
            "Train Epoch: 4 [27000/48000 (56%)]\tLoss: 0.353596\n",
            "Train Epoch: 5 [0/48000 (0%)]\tLoss: 0.251192\n",
            "Train Epoch: 5 [27000/48000 (56%)]\tLoss: 0.280483\n",
            "\n",
            "Validation set: Avg. loss: 0.0011, Accuracy: 11074/12000 (92%)\n",
            "\n",
            "Train Epoch: 6 [0/48000 (0%)]\tLoss: 0.289323\n",
            "Train Epoch: 6 [27000/48000 (56%)]\tLoss: 0.302370\n",
            "Train Epoch: 7 [0/48000 (0%)]\tLoss: 0.228001\n",
            "Train Epoch: 7 [27000/48000 (56%)]\tLoss: 0.284647\n",
            "Train Epoch: 8 [0/48000 (0%)]\tLoss: 0.296632\n",
            "Train Epoch: 8 [27000/48000 (56%)]\tLoss: 0.208509\n",
            "Train Epoch: 9 [0/48000 (0%)]\tLoss: 0.242301\n",
            "Train Epoch: 9 [27000/48000 (56%)]\tLoss: 0.256627\n",
            "Train Epoch: 10 [0/48000 (0%)]\tLoss: 0.219809\n",
            "Train Epoch: 10 [27000/48000 (56%)]\tLoss: 0.237824\n",
            "\n",
            "Validation set: Avg. loss: 0.0010, Accuracy: 11099/12000 (92%)\n",
            "\n",
            "Train Epoch: 11 [0/48000 (0%)]\tLoss: 0.220259\n",
            "Train Epoch: 11 [27000/48000 (56%)]\tLoss: 0.245915\n",
            "Train Epoch: 12 [0/48000 (0%)]\tLoss: 0.314862\n",
            "Train Epoch: 12 [27000/48000 (56%)]\tLoss: 0.155943\n",
            "Train Epoch: 13 [0/48000 (0%)]\tLoss: 0.270567\n",
            "Train Epoch: 13 [27000/48000 (56%)]\tLoss: 0.233261\n",
            "Train Epoch: 14 [0/48000 (0%)]\tLoss: 0.188153\n",
            "Train Epoch: 14 [27000/48000 (56%)]\tLoss: 0.234162\n",
            "\n",
            "Test set: Avg. loss: 0.0003, Accuracy: 9244/10000 (92%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 92 %\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.347353\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.833578\n",
            "\n",
            "Validation set: Avg. loss: 0.0093, Accuracy: 4311/12000 (36%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.831187\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.764694\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.689588\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.878623\n",
            "Train Epoch: 3 [0/38000 (0%)]\tLoss: 1.779913\n",
            "Train Epoch: 3 [20000/38000 (53%)]\tLoss: 1.683853\n",
            "Train Epoch: 4 [0/38000 (0%)]\tLoss: 1.706336\n",
            "Train Epoch: 4 [20000/38000 (53%)]\tLoss: 1.680723\n",
            "Train Epoch: 5 [0/38000 (0%)]\tLoss: 1.716055\n",
            "Train Epoch: 5 [20000/38000 (53%)]\tLoss: 1.782608\n",
            "\n",
            "Validation set: Avg. loss: 0.0088, Accuracy: 4809/12000 (40%)\n",
            "\n",
            "Train Epoch: 6 [0/38000 (0%)]\tLoss: 1.716751\n",
            "Train Epoch: 6 [20000/38000 (53%)]\tLoss: 1.739356\n",
            "Train Epoch: 7 [0/38000 (0%)]\tLoss: 1.697147\n",
            "Train Epoch: 7 [20000/38000 (53%)]\tLoss: 1.540527\n",
            "Train Epoch: 8 [0/38000 (0%)]\tLoss: 1.720183\n",
            "Train Epoch: 8 [20000/38000 (53%)]\tLoss: 1.752718\n",
            "Train Epoch: 9 [0/38000 (0%)]\tLoss: 1.652424\n",
            "Train Epoch: 9 [20000/38000 (53%)]\tLoss: 1.727290\n",
            "Train Epoch: 10 [0/38000 (0%)]\tLoss: 1.733341\n",
            "Train Epoch: 10 [20000/38000 (53%)]\tLoss: 1.862822\n",
            "\n",
            "Validation set: Avg. loss: 0.0087, Accuracy: 4877/12000 (41%)\n",
            "\n",
            "Train Epoch: 11 [0/38000 (0%)]\tLoss: 1.743236\n",
            "Train Epoch: 11 [20000/38000 (53%)]\tLoss: 1.694156\n",
            "Train Epoch: 12 [0/38000 (0%)]\tLoss: 1.677970\n",
            "Train Epoch: 12 [20000/38000 (53%)]\tLoss: 1.645757\n",
            "Train Epoch: 13 [0/38000 (0%)]\tLoss: 1.722740\n",
            "Train Epoch: 13 [20000/38000 (53%)]\tLoss: 1.650213\n",
            "Train Epoch: 14 [0/38000 (0%)]\tLoss: 1.654661\n",
            "Train Epoch: 14 [20000/38000 (53%)]\tLoss: 1.726593\n",
            "\n",
            "Test set: Avg. loss: 0.0017, Accuracy: 4049/10000 (40%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 40 %\n",
            "part 2\n",
            "learning rate list:\n",
            "[0.004907 0.008722 0.000907 0.004911]\n",
            "Lammda list:\n",
            "[0.006292 0.009938 0.006908 0.005694]\n",
            "gen: 0\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.470500\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.431628\n",
            "\n",
            "Validation set: Avg. loss: 0.0014, Accuracy: 10806/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.504219\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.412640\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10918/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.448253\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.389967\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10858/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.008722 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.428810\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.481860\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10736/12000 (89%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "[accuracy,LR_2d] [[0, 0.00491], [0.9005, 0.000907], [0.9048333333333334, 0.004907], [0.9098333333333334, 0.004911]] [accuracy,Lammda_2d] [[0, 0.006371], [0.9005, 0.005694], [0.9048333333333334, 0.006908], [0.9098333333333334, 0.006292]]\n",
            "gen: 1\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.00491 | Lambda =  0.006371\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.373059\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.432659\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10934/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.507904\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.473438\n",
            "\n",
            "Validation set: Avg. loss: 0.0014, Accuracy: 10784/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.475517\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.419671\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10864/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.492541\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.382271\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10898/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[accuracy,LR_2d] [[0, 0.00491], [0.9053333333333333, 0.004907], [0.9111666666666667, 0.00491], [0.9081666666666667, 0.004911]] [accuracy,Lammda_2d] [[0, 0.006361], [0.9053333333333333, 0.006908], [0.9111666666666667, 0.006371], [0.9081666666666667, 0.006292]]\n",
            "gen: 2\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.00491 | Lambda =  0.006361\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.383621\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.301016\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10852/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.446472\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.380987\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10803/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.00491 | Lambda =  0.006371\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.338591\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.458006\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10842/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.479662\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.424213\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10839/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[accuracy,LR_2d] [[0, 0.002455], [0.90325, 0.004911], [0.9035, 0.00491], [0.9043333333333333, 0.00491]] [accuracy,Lammda_2d] [[0, 0.00637], [0.90325, 0.006292], [0.9035, 0.006371], [0.9043333333333333, 0.006361]]\n",
            "gen: 3\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.002455 | Lambda =  0.00637\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.496456\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.367967\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10944/12000 (91%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.364991\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.459906\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10976/12000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.352778\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.355290\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10873/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.405316\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.435588\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10957/12000 (91%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.351877\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.417396\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10825/12000 (90%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.312394\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.369590\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10917/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.00491 | Lambda =  0.006371\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.485220\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.382905\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10911/12000 (91%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.365914\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.409699\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10930/12000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.430521\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.437152\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10912/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.00491 | Lambda =  0.006361\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.431835\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.435570\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10860/12000 (90%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.450990\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.376848\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10768/12000 (90%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.386172\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.344799\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10829/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[accuracy,LR_2d] [[0, 0.00491], [0.9060833333333334, 0.002455], [0.9093333333333333, 0.00491], [0.90975, 0.004911]] [accuracy,Lammda_2d] [[0, 0.006338], [0.9060833333333334, 0.00637], [0.9093333333333333, 0.006371], [0.90975, 0.006292]]\n",
            "learning rate list:\n",
            "[0.004907 0.008722 0.000907 0.004911]\n",
            "Lammda list:\n",
            "[0.006292 0.009938 0.006908 0.005694]\n",
            "gen: 0\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.275687\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.537089\n",
            "\n",
            "Validation set: Avg. loss: 0.0016, Accuracy: 10610/12000 (88%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 88 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.510564\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.418361\n",
            "\n",
            "Validation set: Avg. loss: 0.0013, Accuracy: 10844/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.526542\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.407325\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10864/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008722 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.556423\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.345836\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10926/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "[accuracy,LR_2d] [[0, 0.007944], [0.9036666666666666, 0.004911], [0.9053333333333333, 0.004907], [0.9105, 0.008722]] [accuracy,Lammda_2d] [[0, 0.0073], [0.9036666666666666, 0.006292], [0.9053333333333333, 0.006908], [0.9105, 0.009938]]\n",
            "gen: 1\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.007944 | Lambda =  0.0073\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.516388\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.293537\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10934/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.466090\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.433453\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10881/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.446172\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.359274\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10855/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008722 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.457881\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.501949\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10899/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[accuracy,LR_2d] [[0, 0.008607], [0.90675, 0.004911], [0.9111666666666667, 0.007944], [0.90825, 0.008722]] [accuracy,Lammda_2d] [[0, 0.00976], [0.90675, 0.006292], [0.9111666666666667, 0.0073], [0.90825, 0.009938]]\n",
            "gen: 2\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008607 | Lambda =  0.00976\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.558217\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.417252\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10914/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.418734\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.333179\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10869/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.007944 | Lambda =  0.0073\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.492435\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.343746\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10923/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008722 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.295828\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.419343\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10853/12000 (90%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[accuracy,LR_2d] [[0, 0.008461], [0.90575, 0.004911], [0.91025, 0.007944], [0.9095, 0.008607]] [accuracy,Lammda_2d] [[0, 0.007992], [0.90575, 0.006292], [0.91025, 0.0073], [0.9095, 0.00976]]\n",
            "gen: 3\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008461 | Lambda =  0.007992\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.497429\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.412911\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10930/12000 (91%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.375166\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.421133\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10928/12000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.301550\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.344269\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10980/12000 (92%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.460019\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.531782\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10860/12000 (90%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.428078\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.355438\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10964/12000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.300358\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.292239\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10973/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.007944 | Lambda =  0.0073\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.454576\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.501983\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10937/12000 (91%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.473412\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.474122\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10901/12000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.318947\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.439967\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10942/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008607 | Lambda =  0.00976\n",
            "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.612486\n",
            "Train Epoch: 0 [27000/48000 (56%)]\tLoss: 0.527787\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10909/12000 (91%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.458530\n",
            "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.435140\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10891/12000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.428677\n",
            "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.300942\n",
            "\n",
            "Validation set: Avg. loss: 0.0012, Accuracy: 10933/12000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "[accuracy,LR_2d] [[0, 0.006216], [0.9118333333333334, 0.007944], [0.9144166666666667, 0.004911], [0.915, 0.008461]] [accuracy,Lammda_2d] [[0, 0.007257], [0.9118333333333334, 0.0073], [0.9144166666666667, 0.006292], [0.915, 0.007992]]\n",
            "learning rate list:\n",
            "[0.004907 0.008722 0.000907 0.004911]\n",
            "Lammda list:\n",
            "[0.006292 0.009938 0.006908 0.005694]\n",
            "gen: 0\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.356239\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.818843\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4578/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.346645\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 2.078088\n",
            "\n",
            "Validation set: Avg. loss: 0.0099, Accuracy: 4141/12000 (35%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 34 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.368725\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.952748\n",
            "\n",
            "Validation set: Avg. loss: 0.0098, Accuracy: 4167/12000 (35%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 34 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.008722 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.404311\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 2.434060\n",
            "\n",
            "Validation set: Avg. loss: 0.0123, Accuracy: 3364/12000 (28%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 28 %\n",
            "[accuracy,LR_2d] [[0, 0.004091], [0.34508333333333335, 0.004911], [0.34725, 0.004907], [0.3815, 0.000907]] [accuracy,Lammda_2d] [[0, 0.005851], [0.34508333333333335, 0.006292], [0.34725, 0.006908], [0.3815, 0.005694]]\n",
            "gen: 1\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004091 | Lambda =  0.005851\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.355829\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 2.054528\n",
            "\n",
            "Validation set: Avg. loss: 0.0096, Accuracy: 4006/12000 (33%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 33 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.285678\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.941048\n",
            "\n",
            "Validation set: Avg. loss: 0.0097, Accuracy: 4068/12000 (34%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 33 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.362728\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.925382\n",
            "\n",
            "Validation set: Avg. loss: 0.0098, Accuracy: 4075/12000 (34%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 33 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.336503\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.828421\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4626/12000 (39%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "[accuracy,LR_2d] [[0, 0.004317], [0.339, 0.004911], [0.33958333333333335, 0.004907], [0.3855, 0.000907]] [accuracy,Lammda_2d] [[0, 0.006826], [0.339, 0.006292], [0.33958333333333335, 0.006908], [0.3855, 0.005694]]\n",
            "gen: 2\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004317 | Lambda =  0.006826\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.420912\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 2.060661\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4191/12000 (35%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 34 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.349309\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 2.065313\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4261/12000 (36%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 35 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.393015\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.917604\n",
            "\n",
            "Validation set: Avg. loss: 0.0106, Accuracy: 4052/12000 (34%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 33 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.373179\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.654608\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4391/12000 (37%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 36 %\n",
            "[accuracy,LR_2d] [[0, 0.00403], [0.34925, 0.004317], [0.36591666666666667, 0.000907], [0.3550833333333333, 0.004911]] [accuracy,Lammda_2d] [[0, 0.005862], [0.34925, 0.006826], [0.36591666666666667, 0.005694], [0.3550833333333333, 0.006292]]\n",
            "gen: 3\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.00403 | Lambda =  0.005862\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.390228\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.902938\n",
            "\n",
            "Validation set: Avg. loss: 0.0097, Accuracy: 3984/12000 (33%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.992023\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.963860\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4042/12000 (34%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.938670\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.917174\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4317/12000 (36%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 35 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004317 | Lambda =  0.006826\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.338598\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.859322\n",
            "\n",
            "Validation set: Avg. loss: 0.0097, Accuracy: 4206/12000 (35%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.852934\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.864375\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4366/12000 (36%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.780834\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 2.000182\n",
            "\n",
            "Validation set: Avg. loss: 0.0099, Accuracy: 4154/12000 (35%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 34 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.361763\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.772600\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4640/12000 (39%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.599010\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.833249\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4651/12000 (39%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.708251\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.717912\n",
            "\n",
            "Validation set: Avg. loss: 0.0088, Accuracy: 4691/12000 (39%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 39 %\n",
            "In the logistic_regression_for_tuning optimizerName:  Adam | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.374049\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.968734\n",
            "\n",
            "Validation set: Avg. loss: 0.0099, Accuracy: 4150/12000 (35%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.927959\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 2.044154\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4139/12000 (34%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.754059\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 2.112629\n",
            "\n",
            "Validation set: Avg. loss: 0.0095, Accuracy: 4279/12000 (36%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 35 %\n",
            "[accuracy,LR_2d] [[0, 0.002055], [0.3565833333333333, 0.004911], [0.3909166666666667, 0.000907], [0.35975, 0.00403]] [accuracy,Lammda_2d] [[0, 0.005789], [0.3565833333333333, 0.006292], [0.3909166666666667, 0.005694], [0.35975, 0.005862]]\n",
            "learning rate list:\n",
            "[0.004907 0.008722 0.000907 0.004911]\n",
            "Lammda list:\n",
            "[0.006292 0.009938 0.006908 0.005694]\n",
            "gen: 0\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.310770\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.950232\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4530/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.431170\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.698477\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4510/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.369691\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.887173\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4608/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.008722 | Lambda =  0.009938\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.414558\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.934905\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4344/12000 (36%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 36 %\n",
            "[accuracy,LR_2d] [[0, 0.004091], [0.37583333333333335, 0.004911], [0.384, 0.004907], [0.3775, 0.000907]] [accuracy,Lammda_2d] [[0, 0.005851], [0.37583333333333335, 0.006292], [0.384, 0.006908], [0.3775, 0.005694]]\n",
            "gen: 1\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004091 | Lambda =  0.005851\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.388562\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.918137\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4543/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.405552\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.819234\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4578/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004907 | Lambda =  0.006908\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.355829\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.851688\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4477/12000 (37%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.374820\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.882107\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4517/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "[accuracy,LR_2d] [[0, 0.00479], [0.3764166666666667, 0.000907], [0.3785833333333333, 0.004091], [0.3815, 0.004911]] [accuracy,Lammda_2d] [[0, 0.006262], [0.3764166666666667, 0.005694], [0.3785833333333333, 0.005851], [0.3815, 0.006292]]\n",
            "gen: 2\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.00479 | Lambda =  0.006262\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.371726\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.707780\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4478/12000 (37%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.363151\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.887436\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4538/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004091 | Lambda =  0.005851\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.358494\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.816737\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4653/12000 (39%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.355171\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.927260\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4612/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "[accuracy,LR_2d] [[0, 0.00473], [0.37816666666666665, 0.000907], [0.38775, 0.004091], [0.38433333333333336, 0.004911]] [accuracy,Lammda_2d] [[0, 0.005975], [0.37816666666666665, 0.005694], [0.38775, 0.005851], [0.38433333333333336, 0.006292]]\n",
            "gen: 3\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.00473 | Lambda =  0.005975\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.338139\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.839191\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4506/12000 (38%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.737586\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.753487\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4768/12000 (40%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.746686\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.893221\n",
            "\n",
            "Validation set: Avg. loss: 0.0088, Accuracy: 4726/12000 (39%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 39 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.000907 | Lambda =  0.005694\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.438874\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.947593\n",
            "\n",
            "Validation set: Avg. loss: 0.0091, Accuracy: 4623/12000 (39%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.908464\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.855059\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4739/12000 (39%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.872705\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.865113\n",
            "\n",
            "Validation set: Avg. loss: 0.0088, Accuracy: 4820/12000 (40%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 40 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004091 | Lambda =  0.005851\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.307374\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.856249\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4498/12000 (37%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.675770\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.739837\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4617/12000 (38%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.737144\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.767426\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4605/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "In the logistic_regression_for_tuning optimizerName:  SGD | learning rate =  0.004911 | Lambda =  0.006292\n",
            "Train Epoch: 0 [0/38000 (0%)]\tLoss: 2.387830\n",
            "Train Epoch: 0 [20000/38000 (53%)]\tLoss: 1.919568\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4669/12000 (39%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.650131\n",
            "Train Epoch: 1 [20000/38000 (53%)]\tLoss: 1.762220\n",
            "\n",
            "Validation set: Avg. loss: 0.0090, Accuracy: 4626/12000 (39%)\n",
            "\n",
            "Train Epoch: 2 [0/38000 (0%)]\tLoss: 1.826803\n",
            "Train Epoch: 2 [20000/38000 (53%)]\tLoss: 1.704676\n",
            "\n",
            "Validation set: Avg. loss: 0.0089, Accuracy: 4504/12000 (38%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 37 %\n",
            "[accuracy,LR_2d] [[0, 0.002312], [0.38375, 0.004091], [0.3938333333333333, 0.00473], [0.40166666666666667, 0.000907]] [accuracy,Lammda_2d] [[0, 0.005853], [0.38375, 0.005851], [0.3938333333333333, 0.005975], [0.40166666666666667, 0.005694]]\n",
            "{'MNIST': {'Adam': {'lr': 0.00491, 'lambda': 0.006371, 'best_accuracy': 0.9111666666666667}, 'SGD': {'lr': 0.008461, 'lambda': 0.007992, 'best_accuracy': 0.915}, 'best_optimizer': 'SGD'}, 'CIFAR10': {'Adam': {'lr': 0.000907, 'lambda': 0.005694, 'best_accuracy': 0.3909166666666667}, 'SGD': {'lr': 0.000907, 'lambda': 0.005694, 'best_accuracy': 0.40166666666666667}, 'best_optimizer': 'SGD'}} 0.40166666666666667 846.6022440240013\n",
            "best optimizer for CIFAR10 SGD with learning_rate =  0.000907 Lambda =  0.005694\n",
            "best optimizer for MNIST SGD with learning_rate =  0.008461 Lambda =  0.007992\n",
            "\n",
            "Result:\n",
            " OrderedDict([   (   'MNIST',\n",
            "                    OrderedDict([   ('correct_predict', 9244),\n",
            "                                    ('accuracy', 0.9244),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 114.44197538500066)])),\n",
            "                (   'CIFAR10',\n",
            "                    OrderedDict([   ('correct_predict', 4049),\n",
            "                                    ('accuracy', 0.4049),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 109.88019104999876)])),\n",
            "                ('total_score', 100.0)])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}